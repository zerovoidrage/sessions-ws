// src/hooks/useLocalParticipantTranscription.ts
'use client'

/**
 * –í–ê–ñ–ù–û: –ö–ª–∏–µ–Ω—Ç—Å–∫–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞ –≤ –ø–æ–ª—å–∑—É —Å–µ—Ä–≤–µ—Ä–Ω–æ–π.
 * 
 * –¢–µ–ø–µ—Ä—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ:
 * - –°–µ—Ä–≤–µ—Ä–Ω—ã–π —Ç—Ä–∞–Ω—Å–∫—Ä–∞–π–±–µ—Ä –ø–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è –∫ LiveKit –∫–æ–º–Ω–∞—Ç–µ –ø—Ä–∏ –ø–µ—Ä–µ—Ö–æ–¥–µ —Å–µ—Å—Å–∏–∏ –≤ —Å—Ç–∞—Ç—É—Å LIVE
 * - –ü–æ–¥–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –Ω–∞ –≤—Å–µ –∞—É–¥–∏–æ —Ç—Ä–µ–∫–∏ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤
 * - –ú–∏–∫—à–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –≤ Gladia
 * - –ü—É–±–ª–∏–∫—É–µ—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç—ã —á–µ—Ä–µ–∑ LiveKit data channel
 * 
 * –≠—Ç–æ—Ç —Ö—É–∫ –æ—Å—Ç–∞–≤–ª–µ–Ω –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏, –Ω–æ –±–æ–ª—å—à–µ –Ω–µ –∑–∞—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –º–∏–∫—Ä–æ—Ñ–æ–Ω
 * –∏ –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∞—É–¥–∏–æ –Ω–∞ WebSocket —Å–µ—Ä–≤–µ—Ä.
 * 
 * –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç—ã –ø–æ-–ø—Ä–µ–∂–Ω–µ–º—É –ø—Ä–∏—Ö–æ–¥—è—Ç —á–µ—Ä–µ–∑ LiveKit data channel –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è
 * –≤ useTranscriptStream.ts.
 * 
 * –§—É–Ω–∫—Ü–∏–∏ start() –∏ stop() —Ç–µ–ø–µ—Ä—å no-op - –æ–Ω–∏ –Ω–µ –∑–∞–ø—É—Å–∫–∞—é—Ç —Ä–µ–∞–ª—å–Ω—É—é —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é.
 */

import { useCallback, useEffect, useRef, useState } from 'react'
import { Room, LocalParticipant, Track, ConnectionState } from 'livekit-client'
import { connectTranscriptionWebSocket } from './utils/connectTranscriptionWebSocket'
import type { TranscriptMessage } from '@/types/transcript'
import { clientTranscriptionMetrics } from '@/modules/core/sessions/infra/transcription/transcription-metrics'
import {
  isTranscriptionEnabledForSession,
  canStartTranscriptionForSession,
} from '@/modules/core/sessions/infra/transcription/transcription-flags'

// –§–ª–∞–≥ –¥–ª—è –æ—Ç–∫–ª—é—á–µ–Ω–∏—è –∫–ª–∏–µ–Ω—Ç—Å–∫–æ–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ (—Å–µ—Ä–≤–µ—Ä–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –≤–∫–ª—é—á–µ–Ω–∞)
const SERVER_TRANSCRIPTION_ENABLED = true

interface UseLocalParticipantTranscriptionOptions {
  sessionSlug: string
  room?: Room | null
  localParticipant?: LocalParticipant | null
  connectionState?: ConnectionState
  transcriptionToken?: string // JWT —Ç–æ–∫–µ–Ω –¥–ª—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ WebSocket —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
  isTranscriptionHost?: boolean // true = —ç—Ç–æ—Ç —É—á–∞—Å—Ç–Ω–∏–∫ —è–≤–ª—è–µ—Ç—Å—è designated host –∏ –¥–æ–ª–∂–µ–Ω –∑–∞–ø—É—Å–∫–∞—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é
  userId?: string // ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è —É—á—ë—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
}

export function useLocalParticipantTranscription({
  sessionSlug,
  room,
  localParticipant,
  connectionState = ConnectionState.Disconnected,
  transcriptionToken,
  isTranscriptionHost = false, // –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –Ω–µ host
  userId, // ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è —É—á—ë—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
}: UseLocalParticipantTranscriptionOptions) {
  const [isActive, setIsActive] = useState(false)
  const transcriptionStartedAtRef = useRef<Date | null>(null) // –í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏

  const wsRef = useRef<WebSocket | null>(null)
  const wsReadyRef = useRef(false)
  const audioContextRef = useRef<AudioContext | null>(null)
  const workletNodeRef = useRef<AudioWorkletNode | null>(null) // –ó–∞–º–µ–Ω—è–µ—Ç ScriptProcessorNode
  const sourceRef = useRef<MediaStreamAudioSourceNode | null>(null)
  const mediaStreamRef = useRef<MediaStream | null>(null)
  const mediaStreamTrackRef = useRef<MediaStreamTrack | null>(null)
  const isMountedRef = useRef(true)
  const wsReconnectTimeoutRef = useRef<NodeJS.Timeout | null>(null) // –î–ª—è retry –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ WebSocket
  const audioChunkCountRef = useRef(0) // –°—á–µ—Ç—á–∏–∫ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –∞—É–¥–∏–æ-—á–∞–Ω–∫–æ–≤
  const lastStartAttemptRef = useRef<number>(0) // –í—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø–æ–ø—ã—Ç–∫–∏ –∑–∞–ø—É—Å–∫–∞ (–∑–∞—â–∏—Ç–∞ –æ—Ç —á–∞—Å—Ç—ã—Ö –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–æ–≤)
  const isStartingRef = useRef(false) // –§–ª–∞–≥, —á—Ç–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è
  const localParticipantRef = useRef<LocalParticipant | null>(null)
  const transcriptionTokenRef = useRef<string | undefined>(transcriptionToken) // –°–æ—Ö—Ä–∞–Ω—è–µ–º transcriptionToken –¥–ª—è –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
  const isTranscriptionHostRef = useRef<boolean>(isTranscriptionHost) // –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–ª–∞–≥ host –¥–ª—è –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è

  const onTranscriptCallbackRef = useRef<((message: TranscriptMessage) => void) | null>(null)

  // –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º refs –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏
  useEffect(() => {
    localParticipantRef.current = localParticipant || null
    transcriptionTokenRef.current = transcriptionToken
    isTranscriptionHostRef.current = isTranscriptionHost
  }, [localParticipant, transcriptionToken, isTranscriptionHost])

  // –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–≤ –æ—Ç —Å–µ—Ä–≤–µ—Ä–∞
  const sendTranscriptFromServer = useCallback(
    ({
      text,
      isFinal,
      utteranceId = null,
    }: {
      text: string
      isFinal: boolean
      utteranceId?: string | null
    }) => {
      if (!room || !localParticipant || !text.trim()) {
        return
      }

      if (!text?.trim()) {
        return
      }

      // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–æ–º–Ω–∞—Ç–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∞ –∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω–æ
      if (room.state !== ConnectionState.Connected) {
        console.warn('[Transcription] Room is not connected, skipping data publish', {
          roomState: room.state,
        })
        return
      }

      const timestamp = Date.now()
      const payload = {
        type: 'transcript' as const,
        id: '',
        speakerId: localParticipant.identity,
        speakerName: localParticipant.name ?? localParticipant.identity,
        text: text.trim(),
        isFinal,
        ts: timestamp,
        utterance_id: utteranceId || null,
      }

      const encoded = new TextEncoder().encode(JSON.stringify(payload))

      console.log('[Transcription] üì§ sendTranscriptFromServer called', {
        text: text.substring(0, 100),
        isFinal,
        utteranceId,
        hasCallback: !!onTranscriptCallbackRef.current,
        roomState: room.state,
        hasLocalParticipant: !!localParticipant,
      })

      if (onTranscriptCallbackRef.current) {
        try {
          onTranscriptCallbackRef.current({
            id: '',
            sessionSlug,
            speakerId: localParticipant.identity,
            speakerName: localParticipant.name ?? localParticipant.identity,
            text: text.trim(),
            isFinal,
            timestamp,
            utteranceId: utteranceId || null,
          })
          console.log('[Transcription] ‚úÖ onTranscriptCallback executed successfully')
        } catch (error) {
          console.error('[Transcription] ‚ùå Error in onTranscriptCallback:', error)
        }
      } else {
        console.warn('[Transcription] ‚ö†Ô∏è No onTranscriptCallback set!')
      }

      try {
        // –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–¥ –æ—Ç–ø—Ä–∞–≤–∫–æ–π
        if (room.state === ConnectionState.Connected && localParticipant) {
          localParticipant.publishData(encoded, {
            reliable: true,
          })
          console.log('[Transcription] ‚úÖ Data published to LiveKit room', {
            text: text.substring(0, 50),
            isFinal,
            speakerId: localParticipant.identity,
            speakerName: localParticipant.name,
            roomState: room.state,
            participantCount: 1 + (room.remoteParticipants?.size || 0),
          })
        } else {
          console.warn('[Transcription] Cannot publish data: room not connected or participant unavailable', {
            roomState: room.state,
            hasLocalParticipant: !!localParticipant,
          })
        }
      } catch (e) {
        console.error('[Transcription] Failed to publish data', e)
        // –ù–µ –ø—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º –æ—à–∏–±–∫—É –¥–∞–ª—å—à–µ, —á—Ç–æ–±—ã –Ω–µ –ª–æ–º–∞—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é
      }
    },
    [room, localParticipant, sessionSlug, onTranscriptCallbackRef],
  )

  // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
  useEffect(() => {
    isMountedRef.current = true

    return () => {
      isMountedRef.current = false
    }
  }, [])

  // –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ (–º—è–≥–∫–∞—è –æ—á–∏—Å—Ç–∫–∞)
  const stopTranscription = useCallback(async () => {
    console.log('[Transcription] Stopping transcription...')
    
    // –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–µ—Ä–µ–¥ –æ—Å—Ç–∞–Ω–æ–≤–∫–æ–π
    if (transcriptionStartedAtRef.current && localParticipant) {
      const participantIdentity = localParticipant.identity
      const metrics = clientTranscriptionMetrics.getMetrics(sessionSlug, participantIdentity)
      
      if (metrics && !metrics.endedAt) {
        // –ó–∞–≤–µ—Ä—à–∞–µ–º —Å–µ—Å—Å–∏—é –º–µ—Ç—Ä–∏–∫
        const finalMetrics = clientTranscriptionMetrics.endSession(sessionSlug, participantIdentity)
        
        if (finalMetrics) {
          // –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –ë–î —á–µ—Ä–µ–∑ API
          try {
            await fetch('/api/transcription/usage/save', {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({
                sessionSlug,
                participantIdentity,
                userId,
                startedAt: finalMetrics.startedAt.toISOString(),
                endedAt: finalMetrics.endedAt?.toISOString(),
                durationSeconds: finalMetrics.totalDurationSeconds,
                durationMinutes: finalMetrics.totalTranscriptionMinutes,
                audioChunksSent: finalMetrics.totalAudioChunksSent,
                transcriptsReceived: finalMetrics.totalTranscriptsReceived,
                finalTranscripts: finalMetrics.totalFinalTranscripts,
                partialTranscripts: finalMetrics.totalPartialTranscripts,
                errorsCount: finalMetrics.errors.length,
              }),
            })
            console.log('[Transcription] Usage saved to DB', {
              durationMinutes: finalMetrics.totalTranscriptionMinutes,
            })
          } catch (error) {
            console.error('[Transcription] Failed to save usage:', error)
          }
        }
      }
    }
    
    transcriptionStartedAtRef.current = null
    
    // –û—á–∏—â–∞–µ–º timeout –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
    if (wsReconnectTimeoutRef.current) {
      clearTimeout(wsReconnectTimeoutRef.current)
      wsReconnectTimeoutRef.current = null
    }
    
    // –ó–∞–∫—Ä—ã–≤–∞–µ–º WebSocket
    if (wsRef.current) {
      try {
        // –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å –∫–æ–¥–æ–º 1000 (–Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ –∑–∞–∫—Ä—ã—Ç–∏–µ), —á—Ç–æ–±—ã –Ω–µ —Ç—Ä–∏–≥–≥–µ—Ä–∏—Ç—å –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
        wsRef.current.close(1000, 'Transcription stopped')
      } catch (e) {
        console.warn('[Transcription] Error closing WebSocket:', e)
      }
      wsRef.current = null
    }
    wsReadyRef.current = false

    // –û—Ç–∫–ª—é—á–∞–µ–º AudioWorkletNode
    if (workletNodeRef.current) {
      try {
        workletNodeRef.current.disconnect()
        workletNodeRef.current.port.close() // –ó–∞–∫—Ä—ã–≤–∞–µ–º –ø–æ—Ä—Ç –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏–π
      } catch (e) {
        console.warn('[Transcription] Error disconnecting worklet node:', e)
      }
      workletNodeRef.current = null
    }

    // –û—Ç–∫–ª—é—á–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫
    if (sourceRef.current) {
      try {
        sourceRef.current.disconnect()
      } catch (e) {
        console.warn('[Transcription] Error disconnecting source:', e)
      }
      sourceRef.current = null
    }

    // –ó–∞–∫—Ä—ã–≤–∞–µ–º AudioContext
    if (audioContextRef.current) {
      try {
        audioContextRef.current.close()
      } catch (e) {
        console.warn('[Transcription] Error closing AudioContext:', e)
      }
      audioContextRef.current = null
    }

    // –û—á–∏—â–∞–µ–º MediaStream
    if (mediaStreamRef.current) {
      mediaStreamRef.current.getTracks().forEach(track => track.stop())
      mediaStreamRef.current = null
    }

    // –û—á–∏—â–∞–µ–º ref –Ω–∞ MediaStreamTrack
    mediaStreamTrackRef.current = null
    localParticipantRef.current = null

    console.log('[Transcription] Transcription stopped')
  }, [])

  // –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ mediaStreamTrackRef –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —Ç—Ä–µ–∫–∞
  useEffect(() => {
    if (!localParticipant) return

    const updateMediaStreamTrackRef = () => {
      const micPublication = localParticipant.getTrackPublication(Track.Source.Microphone)
      if (micPublication && micPublication.track) {
        const audioTrack = micPublication.track
        const mediaStreamTrack = (audioTrack as any).mediaStreamTrack as MediaStreamTrack | undefined
        if (mediaStreamTrack) {
          // –í—Å–µ–≥–¥–∞ –æ–±–Ω–æ–≤–ª—è–µ–º ref, –¥–∞–∂–µ –µ—Å–ª–∏ —ç—Ç–æ —Ç–æ—Ç –∂–µ —Ç—Ä–µ–∫ - –µ–≥–æ —Å–≤–æ–π—Å—Ç–≤–∞ –º–æ–≥–ª–∏ –∏–∑–º–µ–Ω–∏—Ç—å—Å—è
          const wasDifferent = mediaStreamTrackRef.current !== mediaStreamTrack
          const oldTrackId = mediaStreamTrackRef.current?.id
          mediaStreamTrackRef.current = mediaStreamTrack
          
          if (wasDifferent) {
            console.log('[Transcription] MediaStreamTrack updated', {
              oldTrackId,
              newTrackId: mediaStreamTrack.id,
              enabled: mediaStreamTrack.enabled,
              muted: mediaStreamTrack.muted,
              readyState: mediaStreamTrack.readyState,
              isMuted: micPublication.isMuted,
            })
            
            // –ï—Å–ª–∏ —Ç—Ä–µ–∫ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∏–∑–º–µ–Ω–∏–ª—Å—è –ò –ø–∞–π–ø–ª–∞–π–Ω —É–∂–µ –∑–∞–ø—É—â–µ–Ω - –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º MediaStream
            if (oldTrackId && oldTrackId !== mediaStreamTrack.id && audioContextRef.current && sourceRef.current) {
              console.log('[Transcription] Track changed, recreating MediaStream and reconnecting source')
              
              try {
                const audioContext = audioContextRef.current
                const workletNode = workletNodeRef.current
                
                // –û—Ç–∫–ª—é—á–∞–µ–º —Å—Ç–∞—Ä—ã–π source –æ—Ç worklet
                if (workletNode) {
                  sourceRef.current.disconnect()
                }
                
                // –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–π MediaStream
                if (mediaStreamRef.current) {
                  mediaStreamRef.current.getTracks().forEach(track => track.stop())
                  mediaStreamRef.current = null
                }
                
                // –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π MediaStream –∏–∑ –Ω–æ–≤–æ–≥–æ —Ç—Ä–µ–∫–∞
                const newMediaStream = new MediaStream([mediaStreamTrack])
                mediaStreamRef.current = newMediaStream
                
                // –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π source –∏–∑ –Ω–æ–≤–æ–≥–æ MediaStream
                const newSource = audioContext.createMediaStreamSource(newMediaStream)
                sourceRef.current = newSource
                
                // –ü–æ–¥–∫–ª—é—á–∞–µ–º –Ω–æ–≤—ã–π source –∫ worklet
                if (workletNode) {
                  newSource.connect(workletNode)
                  console.log('[Transcription] MediaStream and source recreated and reconnected')
                }
              } catch (error) {
                console.error('[Transcription] Failed to recreate MediaStream:', error)
              }
            }
          }
        } else {
          console.warn('[Transcription] No mediaStreamTrack found in audioTrack', {
            hasAudioTrack: !!audioTrack,
            isMuted: micPublication.isMuted,
          })
        }
      } else {
        console.warn('[Transcription] No micPublication or track found when updating ref', {
          hasMicPublication: !!micPublication,
          hasTrack: !!micPublication?.track,
          isMuted: micPublication?.isMuted,
        })
      }
    }

    const handleTrackMuted = () => {
      const micPublication = localParticipant.getTrackPublication(Track.Source.Microphone)
      console.log('[Transcription] Microphone track muted event - audio will be gated in convertAndSendAudio', {
        isMuted: micPublication?.isMuted,
        hasTrack: !!micPublication?.track,
      })
      // –û–±–Ω–æ–≤–ª—è–µ–º ref —Å—Ä–∞–∑—É, —á—Ç–æ–±—ã convertAndSendAudio –≤–∏–¥–µ–ª –∞–∫—Ç—É–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
      updateMediaStreamTrackRef()
      // –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫, —á—Ç–æ–±—ã –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ "Audio gated" —Ä–∞–±–æ—Ç–∞–ª–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
      audioChunkCountRef.current = 0
    }

    const handleTrackUnmuted = () => {
      const micPublication = localParticipant.getTrackPublication(Track.Source.Microphone)
      console.log('[Transcription] Microphone track unmuted event - audio will flow again', {
        isMuted: micPublication?.isMuted,
        hasTrack: !!micPublication?.track,
      })
      // –û–±–Ω–æ–≤–ª—è–µ–º ref —Å—Ä–∞–∑—É, —á—Ç–æ–±—ã convertAndSendAudio –≤–∏–¥–µ–ª –∞–∫—Ç—É–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
      updateMediaStreamTrackRef()
      // –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫, —á—Ç–æ–±—ã –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–ª–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
      audioChunkCountRef.current = 0
    }

    const handleTrackPublished = () => {
      console.log('[Transcription] Microphone track published event')
      updateMediaStreamTrackRef()
    }

    const handleTrackUnpublished = () => {
      console.log('[Transcription] Microphone track unpublished event')
      // –ù–µ –æ–±–Ω—É–ª—è–µ–º ref –ø—Ä–∏ unpublish - —Ç—Ä–µ–∫ –º–æ–∂–µ—Ç –±—ã—Ç—å –µ—â–µ –¥–æ—Å—Ç—É–ø–µ–Ω
    }

    // –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ä–∞–∑—É
    updateMediaStreamTrackRef()

    localParticipant.on('trackMuted', handleTrackMuted)
    localParticipant.on('trackUnmuted', handleTrackUnmuted)
    localParticipant.on('trackPublished', handleTrackPublished)
    localParticipant.on('trackUnpublished', handleTrackUnpublished)

    return () => {
      localParticipant.off('trackMuted', handleTrackMuted)
      localParticipant.off('trackUnmuted', handleTrackUnmuted)
      localParticipant.off('trackPublished', handleTrackPublished)
      localParticipant.off('trackUnpublished', handleTrackUnpublished)
    }
  }, [localParticipant])

  // –ó–∞–ø—É—Å–∫ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
  useEffect(() => {
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞
    // –ë–ª–æ–∫–∏—Ä—É–µ–º –∑–∞–ø—É—Å–∫ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ç—Ä–µ–∫ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ò —è–≤–Ω–æ –≤—ã–∫–ª—é—á–µ–Ω (isMuted === true)
    // –ï—Å–ª–∏ —Ç—Ä–µ–∫–∞ –Ω–µ—Ç - —Ä–∞–∑—Ä–µ—à–∞–µ–º –∑–∞–ø—É—Å–∫ (—Ç—Ä–µ–∫ –ø–æ—è–≤–∏—Ç—Å—è –ø—Ä–∏ –≤–∫–ª—é—á–µ–Ω–∏–∏ –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞)
    const micPublication = localParticipant?.getTrackPublication(Track.Source.Microphone)
    const isMicMuted = micPublication && micPublication.track && micPublication.isMuted

    console.log('[Transcription] useEffect triggered', {
      isActive,
      isTranscriptionHost,
      hasRoom: !!room,
      hasLocalParticipant: !!localParticipant,
      connectionState,
      isMicMuted,
      hasMicPublication: !!micPublication,
      hasMicTrack: micPublication && !!micPublication.track,
    })

    // –ü—Ä–æ–≤–µ—Ä—è–µ–º feature flags
    if (!isTranscriptionEnabledForSession(sessionSlug)) {
      console.log('[Transcription] Transcription disabled for session', { sessionSlug })
      if (wsRef.current || audioContextRef.current) {
        stopTranscription()
      }
      return
    }

    // –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∞–∫—Ç–∏–≤–Ω—ã—Ö —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–π
    // TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É —á–µ—Ä–µ–∑ API –∏–ª–∏ –ª–æ–∫–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    // –ü–æ–∫–∞ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º - –ø—Ä–æ–≤–µ—Ä–∫–∞ –±—É–¥–µ—Ç —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–µ—Ä–≤–µ—Ä–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

    // –í–ê–ñ–ù–û: –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –¥–ª—è –ö–ê–ñ–î–û–ì–û —É—á–∞—Å—Ç–Ω–∏–∫–∞
    // –ö–∞–∂–¥—ã–π —É—á–∞—Å—Ç–Ω–∏–∫ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç —Å–≤–æ–π —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –≥–æ–ª–æ—Å
    // –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç—ã –æ—Ç–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —á–µ—Ä–µ–∑ LiveKit data channel –¥–ª—è –≤—Å–µ—Ö —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤

    // –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –¥–æ–ª–∂–Ω–∞ –ª–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –±—ã—Ç—å –∞–∫—Ç–∏–≤–Ω–∞
    // –ü—Ä–∏ Reconnecting –ù–ï –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é - –æ–Ω–∞ –º–æ–∂–µ—Ç –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å —Ä–∞–±–æ—Ç–∞—Ç—å
    // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–∏ Disconnected –∏–ª–∏ –µ—Å–ª–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –≤—ã–∫–ª—é—á–µ–Ω–∞ (isActive === false)
    // Mute –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞ –ù–ï –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –ø–∞–π–ø–ª–∞–π–Ω - –æ–Ω —Ç–æ–ª—å–∫–æ –±–ª–æ–∫–∏—Ä—É–µ—Ç –æ—Ç–ø—Ä–∞–≤–∫—É –∞—É–¥–∏–æ —á–µ—Ä–µ–∑ convertAndSendAudio
    // –ù–ï —Ç—Ä–µ–±—É–µ–º hasMicTrack - —Ç—Ä–µ–∫ –º–æ–∂–µ—Ç –ø–æ—è–≤–∏—Ç—å—Å—è –ø–æ–∑–∂–µ, –∫–æ–≥–¥–∞ –º–∏–∫—Ä–æ—Ñ–æ–Ω –≤–∫–ª—é—á–∏—Ç—Å—è
    const shouldBeActive = isActive && 
      (connectionState === ConnectionState.Connected || connectionState === ConnectionState.Reconnecting) && 
      !!room && 
      !!localParticipant
    // –ù–ï –ø—Ä–æ–≤–µ—Ä—è–µ–º isMicMuted –∑–¥–µ—Å—å - mute –±–ª–æ–∫–∏—Ä—É–µ—Ç —Ç–æ–ª—å–∫–æ –æ—Ç–ø—Ä–∞–≤–∫—É –∞—É–¥–∏–æ, –Ω–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –ø–∞–π–ø–ª–∞–π–Ω

    // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤—ã–∫–ª—é—á–∏–ª –µ—ë (isActive === false) –∏–ª–∏ disconnected
    // –ù–ï –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏–∑-–∑–∞ mute - mute —Ç–æ–ª—å–∫–æ –±–ª–æ–∫–∏—Ä—É–µ—Ç –ø–æ—Ç–æ–∫ –∞—É–¥–∏–æ
    if (!shouldBeActive) {
      if (wsRef.current || audioContextRef.current) {
        console.log('[Transcription] Stopping transcription due to state change', {
          isActive,
          connectionState,
          hasRoom: !!room,
          hasLocalParticipant: !!localParticipant,
          isMicMuted,
          hasMicPublication: !!micPublication,
        })
        stopTranscription()
      }
      return
    }

    // –ë–ª–æ–∫–∏—Ä—É–µ–º –ó–ê–ü–£–°–ö —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –µ—Å–ª–∏ –º–∏–∫—Ä–æ—Ñ–æ–Ω —è–≤–Ω–æ –≤—ã–∫–ª—é—á–µ–Ω (–Ω–æ –Ω–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —É–∂–µ –∑–∞–ø—É—â–µ–Ω–Ω—É—é)
    if (isMicMuted && !wsRef.current && !audioContextRef.current) {
      console.log('[Transcription] Microphone is muted, not starting transcription pipeline', {
        hasMicPublication: !!micPublication,
        hasTrack: micPublication ? !!micPublication.track : false,
      })
      return
    }

    // –ï—Å–ª–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–µ Reconnecting, –Ω–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —É–∂–µ –∑–∞–ø—É—â–µ–Ω–∞ - –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É
    // WebSocket –º–æ–∂–µ—Ç –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å —Ä–∞–±–æ—Ç–∞—Ç—å –¥–∞–∂–µ –ø—Ä–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º–∞—Ö —Å LiveKit
    // Mute –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞ –Ω–µ –≤–ª–∏—è–µ—Ç - –ø–∞–π–ø–ª–∞–π–Ω –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å
    if (connectionState === ConnectionState.Reconnecting) {
      if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN && audioContextRef.current) {
        console.log('[Transcription] Room reconnecting, but transcription continues', {
          wsState: wsRef.current.readyState,
          hasAudioContext: !!audioContextRef.current,
          isMicMuted,
        })
        // –ù–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é - –æ–Ω–∞ –º–æ–∂–µ—Ç –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å —Ä–∞–±–æ—Ç–∞—Ç—å
        return
      }
    }

    // –ï—Å–ª–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∞–∫—Ç–∏–≤–Ω–∞, –Ω–æ —É–∂–µ –∑–∞–ø—É—â–µ–Ω–∞ –∏ WebSocket –æ—Ç–∫—Ä—ã—Ç, –Ω–∏—á–µ–≥–æ –Ω–µ –¥–µ–ª–∞–µ–º
    // Mute –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞ –Ω–µ –≤–ª–∏—è–µ—Ç - –ø–∞–π–ø–ª–∞–π–Ω –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å, –ø—Ä–æ—Å—Ç–æ –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∞—É–¥–∏–æ
    // –í–ê–ñ–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –ø–∞–π–ø–ª–∞–π–Ω–∞, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞—Ç—å –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ localParticipant
    // –∏–ª–∏ –ø—Ä–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–∏ –¥—Ä—É–≥–∏—Ö —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤
    const isAlreadyRunning = (
      audioContextRef.current &&
      workletNodeRef.current &&
      sourceRef.current &&
      mediaStreamRef.current
    )
    
    // –ï—Å–ª–∏ –ø–∞–π–ø–ª–∞–π–Ω —É–∂–µ –∑–∞–ø—É—â–µ–Ω, –Ω–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º –µ–≥–æ
    // WebSocket –º–æ–∂–µ—Ç –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–∞—Ç—å—Å—è, –Ω–æ —ç—Ç–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ –≤ onclose handler
    if (isAlreadyRunning && shouldBeActive) {
      // –ü—Ä–æ–≤–µ—Ä—è–µ–º WebSocket - –µ—Å–ª–∏ –æ–Ω –∑–∞–∫—Ä—ã—Ç, –Ω–æ –ø–∞–π–ø–ª–∞–π–Ω —Ä–∞–±–æ—Ç–∞–µ—Ç, WebSocket –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–∏—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
      const wsNeedsReconnect = !wsRef.current || 
        (wsRef.current.readyState !== WebSocket.OPEN && wsRef.current.readyState !== WebSocket.CONNECTING)
      
      if (!wsNeedsReconnect) {
        console.log('[Transcription] Already running, skipping restart', {
          isMicMuted,
          wsState: wsRef.current?.readyState,
          hasAudioContext: !!audioContextRef.current,
          hasWorklet: !!workletNodeRef.current,
          hasSource: !!sourceRef.current,
          connectionState,
        })
        // –ù–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º, –¥–∞–∂–µ –µ—Å–ª–∏ localParticipant –∏–∑–º–µ–Ω–∏–ª—Å—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–∏ –¥—Ä—É–≥–∏—Ö —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤)
        return
      } else {
        console.log('[Transcription] Pipeline running but WebSocket needs reconnection, will reconnect separately', {
          wsState: wsRef.current?.readyState,
          hasAudioContext: !!audioContextRef.current,
        })
        // WebSocket –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–∏—Ç—Å—è —á–µ—Ä–µ–∑ –æ—Ç–¥–µ–ª—å–Ω—É—é –ª–æ–≥–∏–∫—É, –Ω–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º –≤–µ—Å—å –ø–∞–π–ø–ª–∞–π–Ω
        return
      }
    }

    const startTranscription = async () => {
      // –ó–∞—â–∏—Ç–∞ –æ—Ç —á–∞—Å—Ç—ã—Ö –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–æ–≤ (–º–∏–Ω–∏–º—É–º 2 —Å–µ–∫—É–Ω–¥—ã –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏)
      const now = Date.now()
      if (isStartingRef.current || (lastStartAttemptRef.current > 0 && now - lastStartAttemptRef.current < 2000)) {
        console.log('[Transcription] Skipping start - too soon after last attempt', {
          isStarting: isStartingRef.current,
          timeSinceLastAttempt: lastStartAttemptRef.current > 0 ? now - lastStartAttemptRef.current : 0,
        })
        return
      }

      isStartingRef.current = true
      lastStartAttemptRef.current = now

      try {
        // –ü–æ–ª—É—á–∞–µ–º –∞—É–¥–∏–æ —Ç—Ä–µ–∫ –æ—Ç LiveKit
        // –ñ–¥–µ–º, –ø–æ–∫–∞ —Ç—Ä–µ–∫ –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω
        let audioTrack = null
        let attempts = 0
        const maxAttempts = 30 // –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–æ 30 –ø–æ–ø—ã—Ç–æ–∫ (6 —Å–µ–∫—É–Ω–¥)

        while (!audioTrack && attempts < maxAttempts) {
          // –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π API LiveKit –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞
          const micPublication = localParticipant.getTrackPublication(Track.Source.Microphone)
          
          // –ü–æ–∑–≤–æ–ª—è–µ–º –∑–∞–ø—É—Å—Ç–∏—Ç—å –ø–∞–π–ø–ª–∞–π–Ω –¥–∞–∂–µ –µ—Å–ª–∏ –º–∏–∫—Ä–æ—Ñ–æ–Ω muted
          // Mute –±—É–¥–µ—Ç –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–ø—Ä–∞–≤–∫—É –∞—É–¥–∏–æ —á–µ—Ä–µ–∑ convertAndSendAudio
          if (micPublication && micPublication.track) {
            audioTrack = micPublication.track
            console.log('[Transcription] Found audio track', {
              trackSid: audioTrack.sid,
              kind: audioTrack.kind,
              isMuted: micPublication.isMuted,
            })
          } else {
            // –ï—Å–ª–∏ —Ç—Ä–µ–∫ –µ—â–µ –Ω–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω, –ø–æ–ø—Ä–æ–±—É–µ–º –≤–∫–ª—é—á–∏—Ç—å –º–∏–∫—Ä–æ—Ñ–æ–Ω
            if (attempts === 5) {
              console.log('[Transcription] Microphone not found, trying to enable...')
              try {
                await localParticipant.setMicrophoneEnabled(true)
              } catch (e) {
                console.warn('[Transcription] Failed to enable microphone:', e)
              }
            }
          }
          
          if (!audioTrack) {
            await new Promise(resolve => setTimeout(resolve, 200))
            attempts++
          }
        }

        if (!audioTrack) {
          console.error('[Transcription] Audio track not available after waiting', {
            attempts,
            hasMicPublication: !!localParticipant.getTrackPublication(Track.Source.Microphone),
            micPublication: localParticipant.getTrackPublication(Track.Source.Microphone),
          })
          isStartingRef.current = false
          return
        }

        console.log('[Transcription] Audio track found', {
          trackSid: audioTrack.sid,
          kind: audioTrack.kind,
        })

        // –ü–æ–ª—É—á–∞–µ–º MediaStreamTrack –∏–∑ —Ç—Ä–µ–∫–∞
        const mediaStreamTrack = (audioTrack as any).mediaStreamTrack as MediaStreamTrack | undefined

        if (!mediaStreamTrack) {
          console.error('[Transcription] No mediaStreamTrack on audioTrack')
          isStartingRef.current = false
          return
        }

        // –°–æ—Ö—Ä–∞–Ω—è–µ–º track –≤ ref, —á—Ç–æ–±—ã —É–º–µ—Ç—å –ø—Ä–æ–≤–µ—Ä—è—Ç—å mute –Ω–∞ —É—Ä–æ–≤–Ω–µ –±—Ä–∞—É–∑–µ—Ä–∞
        mediaStreamTrackRef.current = mediaStreamTrack

        console.log('[Transcription] MediaStreamTrack found', {
          id: mediaStreamTrack.id,
          kind: mediaStreamTrack.kind,
          enabled: mediaStreamTrack.enabled,
          muted: mediaStreamTrack.muted,
          readyState: mediaStreamTrack.readyState,
        })

        // –°–æ–∑–¥–∞–µ–º MediaStream –∏–∑ —Ç—Ä–µ–∫–∞
        const mediaStream = new MediaStream([mediaStreamTrack])
        mediaStreamRef.current = mediaStream

        // –°–æ–∑–¥–∞–µ–º AudioContext —Å –Ω—É–∂–Ω–æ–π sample rate –¥–ª—è Gladia (16kHz)
        const targetSampleRate = 16000
        const audioContext = new AudioContext({
          sampleRate: targetSampleRate,
        })
        audioContextRef.current = audioContext

        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –≤–æ–∑–æ–±–Ω–æ–≤–ª—è–µ–º AudioContext –µ—Å–ª–∏ –æ–Ω suspended (Chrome autoplay policy)
        const resumeAudioContext = async () => {
          if (audioContext.state === 'suspended') {
            try {
              await audioContext.resume()
              console.log('[Transcription] AudioContext resumed')
            } catch (error) {
              console.error('[Transcription] Failed to resume AudioContext:', error)
            }
          }
        }

        // –¢–∞–∫–∂–µ –≤–æ–∑–æ–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–º –∂–µ—Å—Ç–µ (–∫–ª–∏–∫, —Ç–∞—á –∏ —Ç.–¥.)
        const handleUserInteraction = () => {
          resumeAudioContext()
        }
        
        // –ü—ã—Ç–∞–µ–º—Å—è –≤–æ–∑–æ–±–Ω–æ–≤–∏—Ç—å —Å—Ä–∞–∑—É
        resumeAudioContext()

        // –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –¥–ª—è –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–º –∂–µ—Å—Ç–µ
        document.addEventListener('click', handleUserInteraction, { once: true })
        document.addEventListener('touchstart', handleUserInteraction, { once: true })
        
        // –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –¥–ª—è cleanup
        const cleanupUserInteraction = () => {
          document.removeEventListener('click', handleUserInteraction)
          document.removeEventListener('touchstart', handleUserInteraction)
        }

        // –°–æ–∑–¥–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ –∏–∑ MediaStream
        const source = audioContext.createMediaStreamSource(mediaStream)
        sourceRef.current = source

        // –ü–û–î–ö–õ–Æ–ß–ê–ï–ú WEBSOCKET –ü–ï–†–í–´–ú, —á—Ç–æ–±—ã –æ–Ω –±—ã–ª –≥–æ—Ç–æ–≤ –∫ –º–æ–º–µ–Ω—Ç—É, –∫–æ–≥–¥–∞ AudioWorklet –Ω–∞—á–Ω–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å –¥–∞–Ω–Ω—ã–µ
        // –≠—Ç–æ –∏—Å–∫–ª—é—á–∞–µ—Ç –ø—Ä–æ–ø—É—Å–∫ –ø–µ—Ä–≤—ã—Ö –∞—É–¥–∏–æ-—á–∞–Ω–∫–æ–≤
        if (!transcriptionToken) {
          console.error('[Transcription] Missing transcriptionToken, cannot connect to WebSocket')
          throw new Error('Transcription token is required')
        }
        
        // –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ—Ç–æ–∫–æ–ª –∏ —Ö–æ—Å—Ç –¥–ª—è WebSocket
        let wsHost = process.env.NEXT_PUBLIC_WS_HOST || 'localhost'
        wsHost = wsHost.replace(/^https?:\/\//, '').replace(/\/$/, '')
        
        const isProductionHost = wsHost !== 'localhost' && !wsHost.startsWith('127.0.0.1') && !wsHost.startsWith('192.168.')
        const isHttps = typeof window !== 'undefined' && window.location.protocol === 'https:'
        const isProduction = isProductionHost || isHttps
        const wsProtocol = isProduction ? 'wss' : 'ws'
        
        const wsPort = process.env.NEXT_PUBLIC_WS_PORT
        let portSuffix = ''
        // –í–ê–ñ–ù–û: –ï—Å–ª–∏ –ø–æ—Ä—Ç —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –¥–∞–∂–µ –¥–ª—è production
        // –≠—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–ª—è Render –∏ –¥—Ä—É–≥–∏—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º, –≥–¥–µ WebSocket —Å–µ—Ä–≤–µ—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º –ø–æ—Ä—Ç—É
        if (wsPort) {
          portSuffix = `:${wsPort}`
        } else if (!isProduction) {
          // –î–ª—è dev –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Ä—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
          portSuffix = ':3001'
        }
        // –î–ª—è production –±–µ–∑ —è–≤–Ω–æ–≥–æ –ø–æ—Ä—Ç–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ—Ä—Ç (443 –¥–ª—è WSS, –Ω–µ —É–∫–∞–∑—ã–≤–∞–µ–º –≤ URL)
        
        const wsUrl = `${wsProtocol}://${wsHost}${portSuffix}/api/realtime/transcribe?token=${encodeURIComponent(transcriptionToken)}`
        
        console.log('[Transcription] WebSocket URL constructed', {
          wsHost,
          wsProtocol,
          wsPort,
          portSuffix,
          isProduction,
          isProductionHost,
          isHttps,
          wsUrl: wsUrl.replace(/token=[^&]+/, 'token=***'),
        })

        // –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ WebSocket –î–û —Å–æ–∑–¥–∞–Ω–∏—è AudioWorklet
        // –í–ê–ñ–ù–û: –ï—Å–ª–∏ —Å–µ—Ä–≤–µ—Ä–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –≤–∫–ª—é—á–µ–Ω–∞, –∫–ª–∏–µ–Ω—Ç—Å–∫–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞
        if (SERVER_TRANSCRIPTION_ENABLED) {
          console.log('[Transcription] Server transcription enabled, skipping client-side WebSocket connection')
          isStartingRef.current = false
          return // –ù–µ –∑–∞–ø—É—Å–∫–∞–µ–º –∫–ª–∏–µ–Ω—Ç—Å–∫—É—é —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é
        }
        
        let ws: WebSocket
        try {
          ws = await connectTranscriptionWebSocket(wsUrl, {
            maxRetries: 5,
            baseDelayMs: 1000,
            timeoutMs: 10000,
          })
          wsRef.current = ws
          wsReadyRef.current = true
          console.log('[Transcription] ‚úÖ WebSocket connected (before AudioWorklet creation)', {
            wsUrl: wsUrl.replace(/token=[^&]+/, 'token=***'),
            readyState: ws.readyState,
          })
        } catch (error) {
          console.error('[Transcription] Failed to connect WebSocket before AudioWorklet:', {
            error,
            errorMessage: error instanceof Error ? error.message : String(error),
            wsUrl: wsUrl.replace(/token=[^&]+/, 'token=***'),
            hasTranscriptionToken: !!transcriptionToken,
          })
          wsReadyRef.current = false
          wsRef.current = null
          throw error // –ü—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º –æ—à–∏–±–∫—É, —Ç–∞–∫ –∫–∞–∫ –±–µ–∑ WebSocket —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞
        }

        // –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ Float32Array –≤ Int16Array (PCM16) –∏ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ WebSocket
        // –í–ê–ñ–ù–û: –ù–µ –∑–∞–º—ã–∫–∞–µ–º—Å—è –Ω–∞ localParticipant - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ ref –¥–ª—è –∞–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        const convertAndSendAudio = (float32Data: Float32Array) => {
          // 1) WebSocket –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≥–æ—Ç–æ–≤
          const wsState = wsRef.current?.readyState
          if (!wsReadyRef.current || !wsRef.current || wsState !== WebSocket.OPEN) {
            // –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –ø—Ä–æ–ø—É—Å–∫–µ, —á—Ç–æ–±—ã –Ω–µ —Å–ø–∞–º–∏—Ç—å –∫–æ–Ω—Å–æ–ª—å
            if (audioChunkCountRef.current === 0) {
              console.log('[Transcription] Waiting for WebSocket connection...', {
                wsReady: wsReadyRef.current,
                hasWs: !!wsRef.current,
                wsState: wsState,
                wsStateName: wsState === WebSocket.CONNECTING ? 'CONNECTING' :
                            wsState === WebSocket.OPEN ? 'OPEN' :
                            wsState === WebSocket.CLOSING ? 'CLOSING' :
                            wsState === WebSocket.CLOSED ? 'CLOSED' : 'UNKNOWN',
              })
            }
            return
          }

          // 2) –ö–†–ò–¢–ò–ß–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞ –ü–ï–†–ï–î –∫–∞–∂–¥—ã–º —á–∞–Ω–∫–æ–º
          // –í–°–ï–ì–î–ê –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–∫—Ç—É–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ ref (–∫–æ—Ç–æ—Ä–æ–µ –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è —á–µ—Ä–µ–∑ useEffect)
          const participant = localParticipantRef.current
            
          if (!participant) {
            return
          }
          
          const micPublication = participant.getTrackPublication(Track.Source.Microphone)
          
          // –ï—Å–ª–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –Ω–µ—Ç, –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –∞—É–¥–∏–æ (—Ç—Ä–µ–∫ –µ—â–µ –Ω–µ —Å–æ–∑–¥–∞–Ω)
          if (!micPublication) {
            if (audioChunkCountRef.current === 0) {
              console.log('[Transcription] Audio chunk blocked: no micPublication', {
                hasParticipant: !!participant,
              })
            }
            return
          }
          
          // –ì–õ–ê–í–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê: –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞ —á–µ—Ä–µ–∑ –æ–±–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
          // 1. LiveKit TrackPublication.isMuted
          // 2. –ë—Ä–∞—É–∑–µ—Ä–Ω—ã–π MediaStreamTrack (enabled, muted, readyState)
          // –ë–ª–æ–∫–∏—Ä—É–µ–º –æ—Ç–ø—Ä–∞–≤–∫—É –µ—Å–ª–∏ –õ–Æ–ë–û–ô –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ mute
          // –≠—Ç–æ –±–æ–ª–µ–µ –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–æ–¥—Ö–æ–¥: –µ—Å–ª–∏ —Ö–æ—Ç—å –æ–¥–∏–Ω –∏—Å—Ç–æ—á–Ω–∏–∫ –≥–æ–≤–æ—Ä–∏—Ç muted, –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º
          const hasTrack = !!micPublication.track
          const track = mediaStreamTrackRef.current
          
          // –ü—Ä–æ–≤–µ—Ä–∫–∞ LiveKit: isMuted === true –æ–∑–Ω–∞—á–∞–µ—Ç –º–∏–∫—Ä–æ—Ñ–æ–Ω –í–´–ö–õ–Æ–ß–ï–ù
          const liveKitMuted = micPublication.isMuted === true
          
          // –ü—Ä–æ–≤–µ—Ä–∫–∞ MediaStreamTrack: —Ç—Ä–µ–∫ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å enabled, –Ω–µ muted, –∏ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ 'live'
          // –ï—Å–ª–∏ —Ç—Ä–µ–∫ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, —Å—á–∏—Ç–∞–µ–º –µ–≥–æ "–Ω–µ –∂–∏–≤—ã–º"
          const trackIsLive = track && 
            track.enabled === true && 
            track.muted === false && 
            track.readyState === 'live'
          
          // –ë–ª–æ–∫–∏—Ä—É–µ–º –æ—Ç–ø—Ä–∞–≤–∫—É –µ—Å–ª–∏ LiveKit –≥–æ–≤–æ—Ä–∏—Ç muted –ò–õ–ò —Ç—Ä–µ–∫ –Ω–µ –∂–∏–≤–æ–π
          // –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞: –µ—Å–ª–∏ –ª—é–±–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫ –≥–æ–≤–æ—Ä–∏—Ç muted/–Ω–µ –∂–∏–≤–æ–π, –±–ª–æ–∫–∏—Ä—É–µ–º
          const isMicrophoneMuted = liveKitMuted || !trackIsLive
          
          // –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –≤—ã–∑–æ–≤–µ –∏–ª–∏ –ø—Ä–∏ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–π
          if (audioChunkCountRef.current === 0 || (liveKitMuted && trackIsLive) || (!liveKitMuted && !trackIsLive)) {
            console.log('[Transcription] convertAndSendAudio check', {
              hasMicPublication: !!micPublication,
              liveKitIsMuted: micPublication.isMuted,
              hasTrack,
              trackSid: micPublication.trackSid || null,
              trackEnabled: track?.enabled,
              trackMuted: track?.muted,
              trackReadyState: track?.readyState,
              trackIsLive,
              isMicrophoneMuted,
              blockReason: liveKitMuted ? 'LiveKit says muted' : (!trackIsLive ? 'Track not live' : null),
            })
          }
          
          if (!hasTrack) {
            // –¢—Ä–µ–∫–∞ –Ω–µ—Ç - –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –∞—É–¥–∏–æ
            if (audioChunkCountRef.current === 0) {
              console.log('[Transcription] Audio chunk blocked: no track in micPublication', {
                isMuted: micPublication.isMuted,
                hasPublication: !!micPublication,
                trackSid: micPublication.trackSid || null,
                trackEnabled: track?.enabled,
                trackMuted: track?.muted,
                trackReadyState: track?.readyState,
              })
            }
            return
          }
          
          if (isMicrophoneMuted) {
            // –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∏–Ω–æ–≥–¥–∞, —á—Ç–æ–±—ã –Ω–µ —Å–ø–∞–º–∏—Ç—å –∫–æ–Ω—Å–æ–ª—å
            if (audioChunkCountRef.current === 0 || audioChunkCountRef.current % 100 === 0) {
              console.log('[Transcription] ‚ùå Audio chunk BLOCKED: microphone is muted', {
                liveKitIsMuted: micPublication.isMuted,
                hasTrack: !!micPublication.track,
                trackEnabled: track?.enabled,
                trackMuted: track?.muted,
                trackReadyState: track?.readyState,
                trackIsLive,
                blockReason: liveKitMuted ? 'LiveKit muted' : (!trackIsLive ? 'Track not live' : 'Unknown'),
                chunkCount: audioChunkCountRef.current,
              })
            }
            return
          }
          
          // –õ–æ–≥–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–π —É—Å–ø–µ—à–Ω—ã–π —á–∞–Ω–∫ –ø–æ—Å–ª–µ unmute (–∫–æ–≥–¥–∞ —Å—á–µ—Ç—á–∏–∫ –±—ã–ª —Å–±—Ä–æ—à–µ–Ω)
          if (audioChunkCountRef.current === 0) {
            console.log('[Transcription] ‚úÖ Audio chunk ALLOWED: microphone is unmuted', {
              isMuted: micPublication.isMuted,
              hasTrack: !!micPublication.track,
              trackEnabled: track?.enabled,
              trackMuted: track?.muted,
              trackReadyState: track?.readyState,
            })
          }


            // 3) –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º Float32Array –≤ Int16Array (PCM16)
            const pcm16 = new Int16Array(float32Data.length)
            for (let i = 0; i < float32Data.length; i++) {
              const s = Math.max(-1, Math.min(1, float32Data[i]))
              pcm16[i] = s < 0 ? s * 0x8000 : s * 0x7FFF
            }

            // –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã–µ –Ω–∞ —Å–µ—Ä–≤–µ—Ä
            try {
              wsRef.current.send(pcm16.buffer)
              audioChunkCountRef.current++
              
              // –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
              if (localParticipant) {
                clientTranscriptionMetrics.incrementAudioChunks(sessionSlug, localParticipant.identity)
              }
              
              if (audioChunkCountRef.current === 1 || audioChunkCountRef.current % 100 === 0) {
                console.log('[Transcription] Audio chunk sent', {
                  chunkNumber: audioChunkCountRef.current,
                  bufferSize: pcm16.length,
                })
              }
            } catch (error) {
              console.error('[Transcription] Error sending audio:', error)
              // –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ—à–∏–±–∫—É –≤ –º–µ—Ç—Ä–∏–∫–∏
              if (localParticipant) {
                const errorMsg = error instanceof Error ? error.message : String(error)
                clientTranscriptionMetrics.recordError(sessionSlug, localParticipant.identity, errorMsg)
              }
            }
          }

        // –ó–∞–≥—Ä—É–∂–∞–µ–º AudioWorklet –º–æ–¥—É–ª—å –∏ —Å–æ–∑–¥–∞–µ–º AudioWorkletNode
        try {
          // –ó–∞–≥—Ä—É–∂–∞–µ–º worklet processor
          await audioContext.audioWorklet.addModule('/audio/transcription-processor.js')
          console.log('[Transcription] AudioWorklet module loaded')

          // –°–æ–∑–¥–∞–µ–º AudioWorkletNode
          const workletNode = new AudioWorkletNode(audioContext, 'transcription-processor')
          workletNodeRef.current = workletNode

          // –ü–æ–¥–ø–∏—Å—ã–≤–∞–µ–º—Å—è –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ worklet
          workletNode.port.onmessage = (event) => {
            if (event.data?.type === 'audio-data' && event.data?.buffer) {
              // –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ AudioContext (–º–æ–∂–µ—Ç –±—ã—Ç—å suspended –≤ Chrome)
              if (audioContext.state === 'suspended') {
                // –ü—ã—Ç–∞–µ–º—Å—è –≤–æ–∑–æ–±–Ω–æ–≤–∏—Ç—å
                audioContext.resume().catch((error) => {
                  console.error('[Transcription] Failed to resume AudioContext:', error)
                })
                return
              }

              // –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º ArrayBuffer –æ–±—Ä–∞—Ç–Ω–æ –≤ Float32Array
              const float32Data = new Float32Array(event.data.buffer)
              
              // –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º (–ø—Ä–æ–≤–µ—Ä–∫–∞ mute –±—É–¥–µ—Ç –≤–Ω—É—Ç—Ä–∏ convertAndSendAudio)
              convertAndSendAudio(float32Data)
            }
          }

          // –ü–æ–¥–∫–ª—é—á–∞–µ–º worklet –∫ –∏—Å—Ç–æ—á–Ω–∏–∫—É –∏ –∫ destination (–∏–ª–∏ –∫ –≥–ª—É—à–∏—Ç–µ–ª—é)
          source.connect(workletNode)
          workletNode.connect(audioContext.destination) // –ú–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ GainNode —Å gain=0, –µ—Å–ª–∏ –Ω–µ –Ω—É–∂–Ω–æ –≤—ã–≤–æ–¥–∏—Ç—å –∑–≤—É–∫

          console.log('[Transcription] AudioWorkletNode created and connected')
        } catch (error) {
          console.error('[Transcription] Failed to create AudioWorkletNode:', error)
          // Fallback: –µ—Å–ª–∏ AudioWorklet –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è, –º–æ–∂–Ω–æ –≤–µ—Ä–Ω—É—Ç—å—Å—è –∫ ScriptProcessorNode
          // –ù–æ –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞ –ª—É—á—à–µ –ø—Ä–æ—Å—Ç–æ –≤—ã–±—Ä–æ—Å–∏—Ç—å –æ—à–∏–±–∫—É
          throw new Error(`AudioWorklet not supported or failed to load: ${error}`)
        }

        // WebSocket —É–∂–µ –ø–æ–¥–∫–ª—é—á–µ–Ω –≤—ã—à–µ, —Ç–µ–ø–µ—Ä—å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏
        console.log('[Transcription] ‚úÖ WebSocket ready, setting up message handlers', {
          wsUrl: wsUrl.replace(/token=[^&]+/, 'token=***'),
          readyState: ws.readyState,
        })
        
        // –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ WebSocket (–∫–∞–∂–¥—ã–µ 5 —Å–µ–∫—É–Ω–¥)
        const healthCheckInterval = setInterval(() => {
          if (wsRef.current) {
            const state = wsRef.current.readyState
            if (state !== WebSocket.OPEN) {
              console.warn('[Transcription] ‚ö†Ô∏è WebSocket health check failed', {
                state,
                stateName: state === WebSocket.CONNECTING ? 'CONNECTING' :
                          state === WebSocket.OPEN ? 'OPEN' :
                          state === WebSocket.CLOSING ? 'CLOSING' :
                          state === WebSocket.CLOSED ? 'CLOSED' : 'UNKNOWN',
              })
            } else {
              console.log('[Transcription] ‚úÖ WebSocket health check OK', {
                chunkCount: audioChunkCountRef.current,
              })
            }
          }
        }, 5000)
        
        // –û—á–∏—â–∞–µ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏
        ws.addEventListener('close', () => {
          clearInterval(healthCheckInterval)
        })

        // –û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –æ—Ç —Å–µ—Ä–≤–µ—Ä–∞ (–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ onclose, —á—Ç–æ–±—ã –º–æ–∂–Ω–æ –±—ã–ª–æ –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å)
        const handleMessage = (event: MessageEvent) => {
          try {
            // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–æ–º–Ω–∞—Ç–∞ –µ—â–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∞
            if (!room || room.state !== ConnectionState.Connected || !localParticipant) {
              console.warn('[Transcription] Received message but room/participant not ready', {
                hasRoom: !!room,
                roomState: room?.state,
                hasLocalParticipant: !!localParticipant,
              })
              return
            }

            const data = JSON.parse(event.data)
            
            // –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –≤—Ö–æ–¥—è—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
            console.log('[Transcription] üì® WebSocket message received', {
              type: data.type,
              hasText: !!data.text,
              textLength: data.text?.length,
              isFinal: data.is_final,
              utteranceId: data.utterance_id || data.utteranceId,
              rawData: data,
            })

            if (data.type === 'transcription' && data.text?.trim() && isMountedRef.current) {
              const isFinal = Boolean(data.is_final)
              
              console.log('[Transcription] ‚úÖ Processing transcription message', {
                text: data.text.substring(0, 100),
                isFinal,
                utteranceId: data.utterance_id || data.utteranceId || null,
              })
              
              // –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
              if (localParticipant) {
                clientTranscriptionMetrics.incrementTranscripts(
                  sessionSlug,
                  localParticipant.identity,
                  isFinal
                )
              }
              
              sendTranscriptFromServer({
                text: data.text,
                isFinal,
                utteranceId: data.utterance_id || data.utteranceId || null,
              })
            } else if (data.type === 'error') {
              console.error('[Transcription] Server error:', data.message || data)
              // –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ—à–∏–±–∫—É –≤ –º–µ—Ç—Ä–∏–∫–∏
              if (localParticipant) {
                const errorMsg = data.message || 'Unknown server error'
                clientTranscriptionMetrics.recordError(sessionSlug, localParticipant.identity, errorMsg)
              }
            } else {
              // –õ–æ–≥–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞
              console.warn('[Transcription] Unknown message format', {
                type: data.type,
                data: data,
              })
            }
          } catch (error) {
            console.error('[Transcription] Error parsing server message:', error, {
              eventData: event.data,
            })
          }
        }

        ws.onmessage = handleMessage

        // –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫ WebSocket
        ws.onerror = (error) => {
          console.error('[Transcription] WebSocket error:', error)
          wsReadyRef.current = false
          // –ù–µ –≤—ã–∑—ã–≤–∞–µ–º room.disconnect() - WebSocket –Ω–µ –¥–æ–ª–∂–µ–Ω –ª–æ–º–∞—Ç—å LiveKit –∫–æ–º–Ω–∞—Ç—É
        }

        // –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è WebSocket
        // –í–ê–ñ–ù–û: –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ –∑–∞–º—ã–∫–∞–Ω–∏—è, –≤–∫–ª—é—á–∞—è handleMessage –∏ convertAndSendAudio
        const reconnectWebSocket = async () => {
          if (!isMountedRef.current || !isActive) {
            console.log('[Transcription] Skipping WebSocket reconnect - not mounted or not active', {
              isMounted: isMountedRef.current,
              isActive,
            })
            return
          }

          // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–æ–º–Ω–∞—Ç–∞ –≤—Å—ë –µ—â—ë –ø–æ–¥–∫–ª—é—á–µ–Ω–∞
          if (!room || room.state !== ConnectionState.Connected || !localParticipant) {
            console.log('[Transcription] Room not connected, skipping WebSocket reconnect', {
              hasRoom: !!room,
              roomState: room?.state,
              hasLocalParticipant: !!localParticipant,
            })
            return
          }

          // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–∞–π–ø–ª–∞–π–Ω –≤—Å—ë –µ—â—ë —Ä–∞–±–æ—Ç–∞–µ—Ç (AudioContext –∏ Worklet –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∞–∫—Ç–∏–≤–Ω—ã)
          if (!audioContextRef.current || !workletNodeRef.current) {
            console.log('[Transcription] Audio pipeline not running, skipping WebSocket reconnect - will restart full pipeline')
            return
          }

          console.log('[Transcription] Attempting to reconnect WebSocket while keeping audio pipeline running...')
          
          // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ transcriptionToken –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
          if (!transcriptionTokenRef.current) {
            console.error('[Transcription] Missing transcriptionToken for reconnection')
            return
          }
          
          // –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π URL —Å –∞–∫—Ç—É–∞–ª—å–Ω—ã–º transcriptionToken (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ –ª–æ–≥–∏–∫—É, —á—Ç–æ –∏ –≤ startTranscription)
          let reconnectWsHost = process.env.NEXT_PUBLIC_WS_HOST || 'localhost'
          reconnectWsHost = reconnectWsHost.replace(/^https?:\/\//, '').replace(/\/$/, '')
          
          const reconnectIsProductionHost = reconnectWsHost !== 'localhost' && !reconnectWsHost.startsWith('127.0.0.1') && !reconnectWsHost.startsWith('192.168.')
          const reconnectIsHttps = typeof window !== 'undefined' && window.location.protocol === 'https:'
          const reconnectIsProduction = reconnectIsProductionHost || reconnectIsHttps
          const reconnectWsProtocol = reconnectIsProduction ? 'wss' : 'ws'
          
          const reconnectWsPort = process.env.NEXT_PUBLIC_WS_PORT
          let reconnectPortSuffix = ''
          // –í–ê–ñ–ù–û: –ï—Å–ª–∏ –ø–æ—Ä—Ç —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –¥–∞–∂–µ –¥–ª—è production
          // –≠—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–ª—è Render –∏ –¥—Ä—É–≥–∏—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º, –≥–¥–µ WebSocket —Å–µ—Ä–≤–µ—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º –ø–æ—Ä—Ç—É
          if (reconnectWsPort) {
            reconnectPortSuffix = `:${reconnectWsPort}`
          } else if (!reconnectIsProduction) {
            // –î–ª—è dev –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Ä—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            reconnectPortSuffix = ':3001'
          }
          // –î–ª—è production –±–µ–∑ —è–≤–Ω–æ–≥–æ –ø–æ—Ä—Ç–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ—Ä—Ç (443 –¥–ª—è WSS, –Ω–µ —É–∫–∞–∑—ã–≤–∞–µ–º –≤ URL)
          
          const reconnectWsUrl = `${reconnectWsProtocol}://${reconnectWsHost}${reconnectPortSuffix}/api/realtime/transcribe?token=${encodeURIComponent(transcriptionTokenRef.current)}`
          
          // –í–ê–ñ–ù–û: –ï—Å–ª–∏ —Å–µ—Ä–≤–µ—Ä–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –≤–∫–ª—é—á–µ–Ω–∞, –Ω–µ –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–∞–µ–º—Å—è
          if (SERVER_TRANSCRIPTION_ENABLED) {
            console.log('[Transcription] Server transcription enabled, skipping WebSocket reconnection')
            return
          }
          
          try {
            const newWs = await connectTranscriptionWebSocket(reconnectWsUrl, {
              maxRetries: 3, // –ú–µ–Ω—å—à–µ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–∏
              baseDelayMs: 500,
              timeoutMs: 5000,
            })
            
            // –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ä—ã–π WebSocket –¥–ª—è –∑–∞–∫—Ä—ã—Ç–∏—è (–µ—Å–ª–∏ –±—ã–ª)
            const oldWs = wsRef.current
            if (oldWs && oldWs.readyState !== WebSocket.CLOSED) {
              try {
                oldWs.close()
              } catch (e) {
                console.warn('[Transcription] Error closing old WebSocket:', e)
              }
            }
            
            wsRef.current = newWs
            wsReadyRef.current = true
            // –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫, —á—Ç–æ–±—ã –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–ª–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –ø–æ—Å–ª–µ –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
            audioChunkCountRef.current = 0
            console.log('[Transcription] ‚úÖ WebSocket reconnected successfully, audio pipeline continues', {
              wsReady: wsReadyRef.current,
              hasAudioContext: !!audioContextRef.current,
              hasWorklet: !!workletNodeRef.current,
            })

            // –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –¥–ª—è –Ω–æ–≤–æ–≥–æ WebSocket
            newWs.onmessage = handleMessage
            newWs.onerror = (error) => {
              console.error('[Transcription] Reconnected WebSocket error:', error)
              wsReadyRef.current = false
            }
            newWs.onclose = (event) => {
              wsReadyRef.current = false
              console.log('[Transcription] Reconnected WebSocket closed', {
                code: event.code,
                reason: event.reason,
              })
              
              // –ï—Å–ª–∏ –∑–∞–∫—Ä—ã–ª–æ—Å—å –Ω–µ –ø–æ –Ω–∞—à–µ–π –∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–µ (–∫–æ–¥ 1000) –∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∞–∫—Ç–∏–≤–Ω–∞, –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–∞–µ–º—Å—è
              if (event.code !== 1000 && isActive && room && room.state === ConnectionState.Connected && localParticipant && isMountedRef.current && audioContextRef.current) {
                // –û—á–∏—â–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π timeout
                if (wsReconnectTimeoutRef.current) {
                  clearTimeout(wsReconnectTimeoutRef.current)
                }
                // –ü—ã—Ç–∞–µ–º—Å—è –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è —Å–Ω–æ–≤–∞
                wsReconnectTimeoutRef.current = setTimeout(reconnectWebSocket, 1000)
              }
            }
          } catch (error) {
            console.error('[Transcription] Failed to reconnect WebSocket:', error)
            // –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è, –Ω–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∞–∫—Ç–∏–≤–Ω–∞, –ø—Ä–æ–±—É–µ–º –µ—â–µ —Ä–∞–∑
            if (isActive && room && room.state === ConnectionState.Connected && localParticipant && isMountedRef.current && audioContextRef.current) {
              if (wsReconnectTimeoutRef.current) {
                clearTimeout(wsReconnectTimeoutRef.current)
              }
              wsReconnectTimeoutRef.current = setTimeout(reconnectWebSocket, 2000) // –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É
            }
          }
        }

        // –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∑–∞–∫—Ä—ã—Ç–∏—è WebSocket —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ–º
        ws.onclose = (event) => {
          wsReadyRef.current = false
          console.log('[Transcription] ‚ö†Ô∏è WebSocket CLOSED', {
            code: event.code,
            reason: event.reason,
            wasClean: event.wasClean,
            isActive,
            roomState: room?.state,
            hasLocalParticipant: !!localParticipant,
            isMounted: isMountedRef.current,
            hasAudioContext: !!audioContextRef.current,
          })

          // –ï—Å–ª–∏ –∑–∞–∫—Ä—ã–ª–æ—Å—å –Ω–µ –ø–æ –Ω–∞—à–µ–π –∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–µ (–∫–æ–¥ 1000) –∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∞–∫—Ç–∏–≤–Ω–∞, –ø—ã—Ç–∞–µ–º—Å—è –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è
          if (
            event.code !== 1000 && // –ù–µ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ –∑–∞–∫—Ä—ã—Ç–∏–µ
            isActive &&
            room &&
            room.state === ConnectionState.Connected &&
            localParticipant &&
            isMountedRef.current &&
            audioContextRef.current
          ) {
            console.log('[Transcription] üîÑ WebSocket closed unexpectedly, will reconnect...')
            // –û—á–∏—â–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π timeout, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
            if (wsReconnectTimeoutRef.current) {
              clearTimeout(wsReconnectTimeoutRef.current)
            }

            // –ü—ã—Ç–∞–µ–º—Å—è –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è —á–µ—Ä–µ–∑ –Ω–µ–±–æ–ª—å—à—É—é –∑–∞–¥–µ—Ä–∂–∫—É
            wsReconnectTimeoutRef.current = setTimeout(() => {
              console.log('[Transcription] üîÑ Attempting WebSocket reconnection...')
              reconnectWebSocket()
            }, 1000)
          } else {
            console.log('[Transcription] ‚ùå WebSocket closed, but reconnection conditions not met', {
              codeNot1000: event.code !== 1000,
              isActive,
              roomConnected: room?.state === ConnectionState.Connected,
              hasLocalParticipant: !!localParticipant,
              isMounted: isMountedRef.current,
              hasAudioContext: !!audioContextRef.current,
            })
          }
        }
        // –°–±—Ä–æ—Å —Å—á–µ—Ç—á–∏–∫–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
        audioChunkCountRef.current = 0

        // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏
        if (localParticipant) {
          transcriptionStartedAtRef.current = new Date()
          clientTranscriptionMetrics.startSession(sessionSlug, localParticipant.identity)
          console.log('[Transcription] Metrics initialized', {
            sessionSlug,
            participantIdentity: localParticipant.identity,
          })
        }

        console.log('[Transcription] Transcription started with AudioWorklet')
        isStartingRef.current = false
      } catch (error) {
        console.error('[Transcription] Failed to start transcription:', error)
        isStartingRef.current = false
        // –°–±—Ä–∞—Å—ã–≤–∞–µ–º lastStartAttemptRef, —á—Ç–æ–±—ã –º–æ–∂–Ω–æ –±—ã–ª–æ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –ø–æ–ø—ã—Ç–∫—É —á–µ—Ä–µ–∑ 2 —Å–µ–∫—É–Ω–¥—ã
        lastStartAttemptRef.current = 0
      }
    }

        // –ó–∞–ø—É—Å–∫–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∞ –µ—â–µ –Ω–µ –∑–∞–ø—É—â–µ–Ω–∞
        // –ü—Ä–∏ Reconnected –ø—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å (–µ—Å–ª–∏ WebSocket –∑–∞–∫—Ä—ã—Ç)
        let startTimeout: NodeJS.Timeout | null = null
        
        if (connectionState === ConnectionState.Connected) {
          // –ï—Å–ª–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —É–∂–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –∏ WebSocket –æ—Ç–∫—Ä—ã—Ç - –Ω–∏—á–µ–≥–æ –Ω–µ –¥–µ–ª–∞–µ–º
          if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN && audioContextRef.current && workletNodeRef.current) {
            console.log('[Transcription] Already running and connected, no restart needed')
            return
          }

          // –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ —á—Ç–æ –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–∏–ª–∏—Å—å (–µ—Å—Ç—å —Å—Ç–∞—Ä—ã–π WebSocket –∏–ª–∏ AudioContext), –¥–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É
          // –ò–Ω–∞—á–µ –∑–∞–ø—É—Å–∫–∞–µ–º —Å—Ä–∞–∑—É
          const delay = (wsRef.current || audioContextRef.current) ? 500 : 0
          startTimeout = setTimeout(() => {
            // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –≤—Å—ë –µ—â—ë –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∞–∫—Ç–∏–≤–Ω–∞
            if (isActive && connectionState === ConnectionState.Connected && room && localParticipant) {
          // –ï—Å–ª–∏ WebSocket –∑–∞–∫—Ä—ã—Ç –∏–ª–∏ AudioContext –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é
          const needsRestart = !wsRef.current || 
                               wsRef.current.readyState !== WebSocket.OPEN || 
                               !audioContextRef.current || 
                               !workletNodeRef.current
          
          if (needsRestart) {
            console.log('[Transcription] Restarting transcription after reconnected', {
              hasWs: !!wsRef.current,
              wsState: wsRef.current?.readyState,
              hasAudioContext: !!audioContextRef.current,
              hasWorkletNode: !!workletNodeRef.current,
            })
            startTranscription()
          }
        }
      }, delay)
    }

    return () => {
      if (startTimeout) {
        clearTimeout(startTimeout)
      }
      // –ú—è–≥–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø—Ä–∏ —Ä–∞–∑–º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è
      stopTranscription()
    }
  }, [isActive, room, localParticipant, connectionState, sendTranscriptFromServer, sessionSlug, stopTranscription])

  return {
    isActive,
    start: () => {
      setIsActive(true)
    },
    stop: () => {
      setIsActive(false)
    },
    sendTranscriptFromServer,
    setOnTranscriptCallback: (callback: ((message: TranscriptMessage) => void) | null) => {
      onTranscriptCallbackRef.current = callback
    },
  }
}

